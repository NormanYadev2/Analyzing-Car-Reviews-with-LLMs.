{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Necessary Packages"
      ],
      "metadata": {
        "id": "cF-ANt-cnf3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from transformers import logging\n",
        "logging.set_verbosity(logging.WARNING)"
      ],
      "metadata": {
        "id": "n0MynVlGnbcS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the Data"
      ],
      "metadata": {
        "id": "wHHfTwsyno9S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYK91dVzm0xy",
        "outputId": "cf15945f-5fd9-4c04-e4ef-7f0895d694e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Electrical nightmare.  Dealer cost for repairs are thru the roof. But Honda or Toyota', ' I traded out of my Acura due to gas prices, on the rise, as it took premium and it was getting up in miles 94,000 but should have kept the Acura, as I miss the handling on the Acura.  The Azera limited  handling is unstable,and bouncy. I cannot do 120 mph comfortably, as it does not handle the curves very well.  transmission has difficulty shifting at times,  ABS does not work well in the snow,  I slid a lot with new tires.  I get 15 mpg due to my driving habits, and  any one who knows how to drive, and likes to drive, will not like this car.   ', ' Gas mileage is terrible. No front seat drivers side leg room. Not a car to drive if you are over six feet tall. Battery drains after 3-4 days of being parked. Hyundai says that is normal(?) Transmission never knows what gear it wants to be in. No reason to buy this car as long as they are building Accords and Camrys.', ' Warning Do not buy Hyundai Azera. It has engine problem the stupid 10 years warrenty is worthless. I own a 2012 Hyundai Azera it has engine problem I took it to the dealer several times for the same issues. The problem with the car is that it’s shaking it vibrates they call it rough idle when it’s turn it on when you drive it you feel it less but when you stop at a light or park it while the car is still on the entire car is shaking. Long story short they put a brand new engine in my car guess what? I still have the same exact issue with the car. At the end the corporate said there is nothing els they can do.', \" I was a big advocate of my Hyundai azera. I didn't complain about having to wait 6 months to correct the recall on the front seat belt. I didn't complain when the dealers were telling every owner they needed to change the interior air filter? I didn't even complain about the dash cracking. But when I found out that it is a design flaw with the passenger air bag and that hundred of 06 and07 cars have the same problem. That Hyundai changed the style because of the design problem in 08. And, that I can't even get it repaired because of the liability. I complained to Hyundai consumer affairs---they blew me off and won't even return a call! \"]\n",
            "['negative', 'negative', 'negative', 'negative', 'negative']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Load the car reviews dataset\n",
        "file_path = \"Car_Reviews.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "reviews = df['Review'].tolist()\n",
        "real_labels = df['Recommend'].apply(lambda x: \"positive\" if x == \"Yes\" else \"negative\").tolist()\n",
        "\n",
        "\n",
        "print(reviews[:5])\n",
        "print(real_labels[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: sentiment classification"
      ],
      "metadata": {
        "id": "ZdxIQvcZpDwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a sentiment analysis LLM into a pipeline\n",
        "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "predicted_labels = classifier(reviews, batch_size=16, truncation=True)\n",
        "\n",
        "# Load accuracy and F1 score metrics\n",
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# Convert labels to integers\n",
        "references = [1 if label == \"positive\" else 0 for label in real_labels]\n",
        "predictions = [1 if prediction['label'] == \"POSITIVE\" else 0 for prediction in predicted_labels]\n",
        "\n",
        "# Compute metrics\n",
        "accuracy_result = accuracy.compute(references=references, predictions=predictions)['accuracy']\n",
        "f1_result = f1.compute(references=references, predictions=predictions)['f1']\n",
        "\n",
        "print(f\"Accuracy: {accuracy_result:.4f}\")\n",
        "print(f\"F1 result: {f1_result:.4f}\")\n",
        "\n",
        "# Detailed Classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(references, predictions, target_names=[\"negative\", \"positive\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RXpeQbQpR9o",
        "outputId": "19205892-9b5a-4fd4-acf1-a5b1f9f20d6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8711\n",
            "F1 result: 0.8573\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.81      0.97      0.88      5339\n",
            "    positive       0.96      0.77      0.86      5339\n",
            "\n",
            "    accuracy                           0.87     10678\n",
            "   macro avg       0.89      0.87      0.87     10678\n",
            "weighted avg       0.89      0.87      0.87     10678\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Translation"
      ],
      "metadata": {
        "id": "h3FEzWJJz7Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import evaluate\n",
        "\n",
        "# Load translation pipeline (use CPU if GPU gives errors)\n",
        "translator = pipeline(\n",
        "    \"translation_en_to_es\",\n",
        "    model=\"Helsinki-NLP/opus-mt-en-es\",\n",
        "    device=-1   # -1 = CPU, 0 = first GPU\n",
        ")\n",
        "\n",
        "# Translate first 10 reviews\n",
        "sample_reviews = reviews[:10]\n",
        "translated_reviews = [translator(review, truncation=True, max_length=256)[0]['translation_text']\n",
        "                      for review in sample_reviews]\n",
        "\n",
        "\n",
        "# Load reference translations (first 10 lines)\n",
        "with open(\"reference_translations.txt\", 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "references = [[line.strip()] for line in lines[:10]]\n",
        "\n",
        "# Compute BLEU score\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_score = bleu.compute(predictions=translated_reviews, references=references)\n",
        "\n",
        "print(f\"\\nBLEU score (first 10 reviews): {bleu_score['bleu']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy8MisQf3uU4",
        "outputId": "d517f6ba-bc2f-4b49-d65e-844b678ae2bf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your input_length: 512 is bigger than 0.9 * max_length: 256. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU score (first 10 reviews): 0.8647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3: Generative QA"
      ],
      "metadata": {
        "id": "tStz50UiBAP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import evaluate\n",
        "\n",
        "# Load generative QA pipeline\n",
        "qa_gen = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=-1)\n",
        "\n",
        "context = reviews[1]\n",
        "question = \"What did the customer like or dislike about the car?\"\n",
        "\n",
        "# Format input\n",
        "input_text = f\"question: {question} context: {context}\"\n",
        "result = qa_gen(input_text, max_new_tokens=100, do_sample=False)\n",
        "\n",
        "predicted_answer = result[0]['generated_text']\n",
        "print(\"Predicted Answer:\", predicted_answer)\n",
        "\n",
        "# ---- Reference Answer (gold label, you need to provide this) ----\n",
        "reference_answer = \"the Azera limited handling is unstable,and bouncy. I cannot do 120 mph comfortably, as it does not handle the curves very well. transmission has difficulty shifting at times, ABS does not work well in the snow, I slid a lot with new tires.\"\n",
        "\n",
        "# ---- BLEU ----\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_score = bleu.compute(predictions=[predicted_answer], references=[[reference_answer]])\n",
        "print(\"BLEU Score:\", bleu_score[\"bleu\"])\n",
        "\n",
        "# ---- ROUGE ----\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_score = rouge.compute(predictions=[predicted_answer], references=[reference_answer])\n",
        "print(\"ROUGE Score:\", rouge_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBdHrlvfBDY5",
        "outputId": "f731f4a1-e8a3-4207-fbfc-1c34c1f133d4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Answer: the Azera limited handling is unstable,and bouncy. I cannot do 120 mph comfortably, as it does not handle the curves very well. transmission has difficulty shifting at times, ABS does not work well in the snow, I slid a lot with new tires. I get 15 mpg due to my driving habits, and any one who knows how to drive, and likes to drive, will not like this car\n",
            "BLEU Score: 0.6304763103210652\n",
            "ROUGE Score: {'rouge1': np.float64(0.7719298245614035), 'rouge2': np.float64(0.7678571428571428), 'rougeL': np.float64(0.7719298245614035), 'rougeLsum': np.float64(0.7719298245614035)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Extractive QA"
      ],
      "metadata": {
        "id": "FLvYGU-pKHuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import evaluate\n",
        "\n",
        "# Load extractive QA pipeline\n",
        "qa_extract = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"deepset/minilm-uncased-squad2\",\n",
        "    tokenizer=\"deepset/minilm-uncased-squad2\",\n",
        "    device=-1   # CPU\n",
        ")\n",
        "\n",
        "context = reviews[1]\n",
        "question = \"What did the customer like or dislike about the car?\"\n",
        "\n",
        "# Run QA\n",
        "result = qa_extract(question=question, context=context, handle_impossible_answer=True)\n",
        "print(\"Answer:\", result['answer'], \"(score:\", f\"{result['score']:.4f})\")\n",
        "\n",
        "# Load metric\n",
        "metric = evaluate.load(\"squad\")\n",
        "\n",
        "# Predictions (no `no_answer_probability`)\n",
        "predictions = [{\"id\": \"0\", \"prediction_text\": result['answer']}]\n",
        "\n",
        "# References\n",
        "references = [{\"id\": \"0\", \"answers\": {\"text\": [\"handling on the Acura\"], \"answer_start\": [0]}}]\n",
        "\n",
        "# Compute EM & F1\n",
        "results = metric.compute(predictions=predictions, references=references)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtcuQKLYJ-ju",
        "outputId": "371a613c-7db1-4604-aace-f187dcdcadaf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: will not like this car (score: 0.1854)\n",
            "{'exact_match': 0.0, 'f1': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task5: Summarization"
      ],
      "metadata": {
        "id": "rUJRYj6hgFd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import evaluate\n",
        "\n",
        "# Load summarization pipeline\n",
        "model_name = \"cnicu/t5-small-booksum\"\n",
        "summarizer = pipeline(\"summarization\", model=model_name,device=-1)\n",
        "\n",
        "# Pick review\n",
        "text_to_summarize = reviews[-1]\n",
        "print(f\"Original text:\\n{text_to_summarize}\\n\")\n",
        "\n",
        "# Generate summary\n",
        "outputs = summarizer(text_to_summarize, max_length=53)\n",
        "summarized_text = outputs[0]['summary_text']\n",
        "print(f\"Model Summary:\\n{summarized_text}\\n\")\n",
        "\n",
        "# ---- Reference summary (gold summary you need to provide) ----\n",
        "reference_summary = \"I love my car. It is fun to drive, has great pick up, handles well. I have no problems on busy freeways. It's very comfortable to take on long trips.\"\n",
        "\n",
        "# ---- BLEU ----\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_score = bleu.compute(predictions=[summarized_text], references=[[reference_summary]])\n",
        "print(\"BLEU Score:\", bleu_score[\"bleu\"])\n",
        "\n",
        "# ---- ROUGE ----\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_score = rouge.compute(predictions=[summarized_text], references=[reference_summary])\n",
        "print(\"ROUGE Score:\", rouge_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CULrPdJWhepG",
        "outputId": "9b5c5572-51c3-42b7-c4bc-3f037a276552"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 53, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=53) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            " I love my car. It is fun to drive, has \rgreat pick up, handles well. I have no \rproblems on busy freeways. It is very \rcomfortable to take on long trips, as \rlong as there are not more than 2 \rpeople.\n",
            "\n",
            "Model Summary:\n",
            "I love my car. It is fun to drive, has great pick up, handles well. I have no problems on busy freeways. It's very comfortable to take on long trips, as long as there are not more than 2 people.\n",
            "\n",
            "BLEU Score: 0.7412780049783892\n",
            "ROUGE Score: {'rouge1': np.float64(0.8611111111111112), 'rouge2': np.float64(0.8571428571428571), 'rougeL': np.float64(0.8611111111111112), 'rougeLsum': np.float64(0.8611111111111112)}\n"
          ]
        }
      ]
    }
  ]
}